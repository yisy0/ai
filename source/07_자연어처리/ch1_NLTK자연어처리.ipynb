{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f8826e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:99% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
       "div.text_cell_render.rendered_html{font-size:20pt;}\n",
       "div.text_cell_render ul li, div.text_cell_render ol li p, code{font-size:22pt; line-height:30px;}\n",
       "div.output {font-size:24pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:24pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
       "table.dataframe{font-size:24px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:99% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
    "div.text_cell_render.rendered_html{font-size:20pt;}\n",
    "div.text_cell_render ul li, div.text_cell_render ol li p, code{font-size:22pt; line-height:30px;}\n",
    "div.output {font-size:24pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:24pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
    "table.dataframe{font-size:24px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6999596",
   "metadata": {},
   "source": [
    "**<font size=\"6\" color=\"red\">ch1. NLTK 자연어처리 패키지</font>**\n",
    "# <span style=\"color:red\">ch1. NLTK 자연어처리 패키지</span>\n",
    "```\n",
    "자연어처리 : 텍스트전처리, 단어의 빈도수 측정, 문서유사도 측정, 연관분석, 딥러닝, \n",
    "            워드임베딩, 텍스트분류, 어휘 데이터 베이스 사용, 감성분석, 분류분석\n",
    "```\n",
    "# 1. NLTK 패키지\n",
    "- 텍스트 전처리 : 토큰화(문장, 어절, 형태소 나누기), 정규표현식을 이용하여 불용어 처리, 어간추출\n",
    "- 품사태깅 : 단어별 품사를 식별\n",
    "```\n",
    "pip install nltk==3.7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b680c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8903695f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 말뭉치 다운로드 받을 폴더\n",
    "# c:/nltk_data, d:/nltk_data, e:/nltk_data,\n",
    "# c:/Users/내컴퓨터이름/nltk_data\n",
    "# c:/Users/내컴퓨터이름/anaconda3/nltk_data,\n",
    "# c:/Users/내컴퓨터이름/anaconda3/share/nltk_data\n",
    "# c:/Users/내컴퓨터이름/anaconda3/lib/nltk_data\n",
    "# c:/User/내컴퓨터이름/AppData/Roaming/nltk_data\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c36f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to d:\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47543ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 말뭉치 리스트\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc84cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VO\n"
     ]
    }
   ],
   "source": [
    "# 오스틴 소석 엠마 내용 데이터 셋\n",
    "emma = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "print(emma[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d6c99cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, 887071)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emma), len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81348b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장수 : 7456\n",
      "'[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.'\n"
     ]
    }
   ],
   "source": [
    "# sent_tokenize() : 문장 단위로 쪼갠 list 반환\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokens = sent_tokenize(emma)\n",
    "print('문장수 :', len(sent_tokens))\n",
    "print(\"%r\" % sent_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac5d0769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize() : 단어 단위로 쪼갠 list 반환\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(sent_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fdae223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1816', '8', '23', '28', '28', '24', '7', '10', '000', '10', '000', '26']\n",
      "['Emma', 'by', 'Jane', 'Austen', '1816', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her']\n",
      "['Emma', 'by', 'Jane', 'Austen', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her']\n"
     ]
    }
   ],
   "source": [
    "# RegexpTokenizer 클래스 : 토큰화할 때, 정규표현식\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "ret = RegexpTokenizer('\\d+') # [0-9] : \\d\n",
    "digits = ret.tokenize(emma)\n",
    "print(digits)\n",
    "ret1 = RegexpTokenizer('\\w+') # [a-zA-Z0-9_] : \\w\n",
    "words = ret1.tokenize(sent_tokens[0])\n",
    "print(words)\n",
    "ret1 = RegexpTokenizer('[a-zA-Z]+') #  해당 정규표현식에 맞는 단어만 추출\n",
    "words = ret1.tokenize(sent_tokens[0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a442b",
   "metadata": {},
   "source": [
    "# 2. 형태소(의미가 있는 가장 작은 단위) 분석\n",
    "cf. 자연어 처리의 기본은 형태소 분석과 품사태깅 - 어간추출(Stemming), 품사태깅(PosTagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b02fd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send', 'cook', 'file', 'live', 'cri', 'die']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['sending', 'cooking', 'files', 'lives', 'crying', 'dying']\n",
    "# 어간 추출1 : PosterStemmer\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, RegexpStemmer\n",
    "pst = PorterStemmer()\n",
    "# pst.stem(words[0], words[1])\n",
    "[pst.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30fabfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send', 'cook', 'fil', 'liv', 'cry', 'dying']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어간 추출2 : LancasterStemmer \n",
    "lst = LancasterStemmer()\n",
    "[lst.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19e49b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send', 'cook', 'files', 'lives', 'cry', 'dy']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어간 추출3 : RegexpStemmer\n",
    "rst = RegexpStemmer('ing')\n",
    "[rst.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e37aa7",
   "metadata": {},
   "source": [
    "# 3. 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a68dfc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사태깅 결과 : [('[', 'NNS'), ('Emma', 'NNP'), ('by', 'IN'), ('Jane', 'NNP'), ('Austen', 'NNP'), ('1816', 'CD'), (']', 'NNP'), ('VOLUME', 'NNP'), ('I', 'PRP'), ('CHAPTER', 'VBP'), ('I', 'PRP'), ('Emma', 'NNP'), ('Woodhouse', 'NNP'), (',', ','), ('handsome', 'NN'), (',', ','), ('clever', 'NN'), (',', ','), ('and', 'CC'), ('rich', 'JJ'), (',', ','), ('with', 'IN'), ('a', 'DT'), ('comfortable', 'JJ'), ('home', 'NN'), ('and', 'CC'), ('happy', 'JJ'), ('disposition', 'NN'), (',', ','), ('seemed', 'VBD'), ('to', 'TO'), ('unite', 'VB'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('blessings', 'NNS'), ('of', 'IN'), ('existence', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), ('lived', 'VBN'), ('nearly', 'RB'), ('twenty-one', 'CD'), ('years', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('with', 'IN'), ('very', 'RB'), ('little', 'JJ'), ('to', 'TO'), ('distress', 'VB'), ('or', 'CC'), ('vex', 'VB'), ('her', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# print('품사태깅할 내용 :', word_tokenize(sent_tokens[0]))\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "tagged_list = pos_tag(word_tokenize(sent_tokens[0]))\n",
    "print('품사태깅 결과 :', tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1e6c5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'Jane', 'Austen', ']', 'VOLUME', 'Emma', 'Woodhouse', 'handsome', 'clever', 'home', 'disposition', 'blessings', 'existence', 'years', 'world']\n"
     ]
    }
   ],
   "source": [
    "# 명사만 추출 : NN, NNS, NNP, NNPS\n",
    "nouns_list = []\n",
    "for word, tag in tagged_list:\n",
    "    #if (tag=='NN') | (tag=='NNS') | (tag=='NNP') |(tag=='NNPS') :\n",
    "    #if tag in ['NN', 'NNS', 'NNP', 'NNPS'] :\n",
    "    if tag.find('NN') != -1:\n",
    "        nouns_list.append(word)\n",
    "print(nouns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "caa7393f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clever', 'Emma', '[', 'years', 'handsome', ']', 'disposition', 'existence', 'world', 'blessings', 'Austen', 'VOLUME', 'home', 'Jane', 'Woodhouse'}\n"
     ]
    }
   ],
   "source": [
    "nouns_list = [word for word, tag in tagged_list if tag.find('NN') != -1]\n",
    "print(nouns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aa73be",
   "metadata": {},
   "source": [
    "## Quiz. emma 소설안에서\n",
    "1. 특수문자가 들어가지 않은 3글자 이상의 단어만 추출해서 품사태깅을 하시오(RegexpTokenizer).\n",
    "\n",
    "2. \"Emma\" 단어가 몇번 등장했는지, 품사태깅이 어떤 품사들로 되어 있는지 모두 출력하시오\n",
    "\n",
    "3. emma 소설에서내가 원하는 품사(명사:NN, NNS, NNP, NNPS)만 뽑아 등장하는 명사의 종류 갯수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e96eb74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma 소설의 글자 수 : 887071\n",
      "emma 소설의 3글자 이상의 단어 수 : 123877\n",
      "123877\n",
      "단어의 종류 수 : 7630 11678\n"
     ]
    }
   ],
   "source": [
    "#1.특수문자가 들어가지 않은 3글자 이상의 단어만 추출해서 품사태깅\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "tokenizer = RegexpTokenizer('\\w{3,}')\n",
    "emma_words = tokenizer.tokenize(emma)\n",
    "emma_tags  = pos_tag(emma_words)\n",
    "print('emma 소설의 글자 수 :', len(emma))\n",
    "print('emma 소설의 3글자 이상의 단어 수 :', len(emma_words))\n",
    "print(len(emma_tags))\n",
    "print('단어의 종류 수 :', len(set(emma_words)), len(set(emma_tags)))\n",
    "# 한 단어가 다른 품사로 태깅될 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5005ce77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Emma', 'NNP'),\n",
       " ('Jane', 'NNP'),\n",
       " ('Austen', 'NNP'),\n",
       " ('1816', 'CD'),\n",
       " ('VOLUME', 'NNP')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma_tags[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8847f80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma 등장 횟수 : 865\n",
      "Emma가 분류된 품사 : {'NNS', 'VBD', 'NNPS', 'VBN', 'NN', 'VBP', 'NNP', 'VB', 'RB', 'JJ'}\n"
     ]
    }
   ],
   "source": [
    "# 2.\"Emma\" 단어가 몇번 등장했는지, 품사태깅이 Emma가 어떤 품사들로 되어 있는지 모두 출력\n",
    "# pos = set() # Emma의 품사들\n",
    "# cnt = 0 # Emma 등장 횟수\n",
    "# for word, tag in emma_tags:\n",
    "#     if word=='Emma':\n",
    "#         cnt += 1\n",
    "#         pos.add(tag)\n",
    "tags = [tag for word, tag in emma_tags if word=='Emma']\n",
    "cnt = len(tags)\n",
    "pos = set(tags)\n",
    "print('Emma 등장 횟수 :', cnt)\n",
    "print('Emma가 분류된 품사 :', pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4402fa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NNP     838\n",
       "NN        7\n",
       "VB        5\n",
       "VBP       4\n",
       "JJ        4\n",
       "NNS       2\n",
       "NNPS      2\n",
       "VBD       1\n",
       "VBN       1\n",
       "RB        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Emma로 분류된 품사의 갯수를 sort 출력\n",
    "import pandas as pd\n",
    "pos_cnt = pd.Series([0]*len(pos), index=pos)\n",
    "for word, tag in emma_tags:\n",
    "    if word == 'Emma':\n",
    "        pos_cnt[tag] += 1\n",
    "pos_cnt.sort_values(ascending=False) #내림차순 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55059b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사가 등장할 횟수 : 30781\n",
      "등장한 명사의 수(중복제거) : 4165\n",
      "한단어가 나오는 평균 빈도수 : 7.3903961584633855\n"
     ]
    }
   ],
   "source": [
    "# 3.emma에서 내가 원하는 품사(명사:NN, NNS, NNP, NNPS)만 뽑아 등장하는 명사의 종류 갯수 출력\n",
    "tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "# nouns_list = []\n",
    "# for token, tag in emma_tags:\n",
    "#     if tag in tags:\n",
    "#         nouns_list.append(token)\n",
    "nouns_list = [token for token, tag in emma_tags if tag in tags]\n",
    "print('명사가 등장할 횟수 :', len(nouns_list))\n",
    "print('등장한 명사의 수(중복제거) :', len(set(nouns_list)))\n",
    "print('한단어가 나오는 평균 빈도수 :', len(nouns_list)/len(set(nouns_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf3f7d",
   "metadata": {},
   "source": [
    "# 4. 최빈 단어 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cad581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7d938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e1f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4e3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf25c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9716465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c80414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb370dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5e0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2693722f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-nlp(ipykernel)",
   "language": "python",
   "name": "ml-dl-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
