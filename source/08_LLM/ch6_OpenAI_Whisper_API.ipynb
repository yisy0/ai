{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b4aa7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:99% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
       "div.text_cell_render.rendered_html{font-size:20pt;}\n",
       "div.text_cell_render li, div.text_cell_render p, code{font-size:22pt; line-height:40px;}\n",
       "div.output {font-size:24pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:24pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
       "table.dataframe{font-size:24px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:99% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
    "div.text_cell_render.rendered_html{font-size:20pt;}\n",
    "div.text_cell_render li, div.text_cell_render p, code{font-size:22pt; line-height:40px;}\n",
    "div.output {font-size:24pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:24pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
    "table.dataframe{font-size:24px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace62d34",
   "metadata": {},
   "source": [
    "# OpenAI Whisper API를 활용한 음성-텍스트 변환 튜토리얼 (2025년 3월 기준)\n",
    "\n",
    "OpenAI의 Whisper 모델은 사람의 음성을 높은 정확도로 텍스트로 변환하는 자동 음성 인식(ASR) 모델입니다. 2025년 현재 Whisper는 OpenAI API를 통해 제공되며, 음성 파일을 원어 그대로 **텍스트로 필사(transcription)**하거나 영어로 **번역(translation)**하는 기능을 제공합니다. 이 튜토리얼에서는 Whisper API를 사용하여 음성-텍스트 변환을 구현하는 방법을 단계별로 살펴보겠습니다. 환경 설정부터 기본 사용 방법, 그리고 언어 감지, 타임스탬프 포함 출력, 실시간 음성 처리와 같은 고급 기능까지 다룹니다.\n",
    "\n",
    "## 1. 환경 설정\n",
    "\n",
    "Python 코드에서 python-dotenv로 .env 파일을 불러온 뒤, openai.OpenAI() 클래스를 이용해 API 클라이언트 인스턴스를 생성합니다. 이때 API 키는 명시적으로 전달하거나 환경 변수 OPENAI_API_KEY가 설정되어 있어야 합니다. 환경 변수 또는 인자로 API 키를 지정하지 않으면 OpenAI 라이브러리는 다음과 같은 오류를 발생시킵니다:\n",
    "\n",
    "> OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
    "\n",
    "아래는 환경 로드 및 클라이언트 생성 예제입니다:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6abb0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "load_dotenv()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf77a5f",
   "metadata": {},
   "source": [
    "위 코드에서는 .env에서 불러온 API 키로 client 객체를 생성했습니다. 이제 이 client를 통해 DALL-E를 비롯한 OpenAI API 요청을 보낼 수 있습니다. (참고로, api_key를 생략하면 OPENAI_API_KEY 환경 변수를 자동으로 참조합니다.)\n",
    "\n",
    "## 2. Whisper API 소개 및 기본 음성 → 텍스트 변환\n",
    "\n",
    "OpenAI Whisper API는 두 가지 주요 **엔드포인트(endpoint)**를 제공합니다:\n",
    "- Transcriptions 엔드포인트: 제공한 음성 파일을 해당 음성의 원어로 필사(글로 변환) 합니다 (예: 한국어 음성을 입력하면 한국어 텍스트로 출력).\n",
    "- Translations 엔드포인트: 제공한 음성 파일을 영어로 번역하여 텍스트로 반환합니다.\n",
    "\n",
    "Whisper API는 현재 Whisper v2 대형 모델(whisper-1로 식별)을 사용하며, MP3, MP4, WAV 등 다양한 음성 파일 형식을 지원합니다.\n",
    "다만 한 번 요청할 수 있는 파일 크기는 최대 25MB로 제한되어 있으므로 긴 오디오의 경우 분할하여 처리해야 합니다.\n",
    "\n",
    "우선 MP3 파일 등의 정적 파일을 Whisper로 **텍스트 변환(필사)**하는 기본 방법을 살펴보겠습니다. 예제로 간단한 영어 음성 MP3 파일을 텍스트로 변환해보겠습니다.\n",
    "\n",
    "### 기본 사용법: 음성 파일 필사 요청\n",
    "1. 오디오 파일 준비: 변환하고자 하는 음성 파일의 경로를 지정합니다. Whisper API는 여러 형식의 오디오를 지원하는데, 일반적으로 wav나 mp3를 많이 사용합니다. 여기서는 예시로 speech.mp3 파일을 사용하겠습니다.\n",
    "2. API 요청 구성: OpenAI Python SDK의 client.audio.transcriptions.create 메서드를 사용하여 음성 필사 요청을 보냅니다. 주요 파라미터:\n",
    "    - file: 열어둔 오디오 파일 객체 (\"rb\" 모드로 연 파일),\n",
    "    - model: 사용할 모델 ID (\"whisper-1\"로 지정하여 Whisper 모델 사용).\n",
    "\n",
    "    기본적으로 Whisper API는 응답으로 JSON 형식 데이터를 반환하며, 그 안에 \"text\" 필드로 변환된 텍스트를 제공합니다. response_format 파라미터를 사용해 응답 형식을 변경할 수 있는데, 기본값 \"json\" 외에 \"text\", \"srt\" 등 다양한 옵션이 있습니다. 우선 이해를 돕기 위해 응답을 순수 텍스트로 받도록 response_format=\"text\"를 지정해보겠습니다.\n",
    "\n",
    "3. 응답 처리: Whisper API는 음성 내용을 텍스트로 변환하여 반환합니다. response_format=\"text\"로 요청한 경우 변환된 텍스트 문자열을 직접 반환하므로, 이를 출력하거나 변수에 저장할 수 있습니다.\n",
    "\n",
    "다음은 MP3 파일을 읽어서 Whisper로 전송하고, 결과 텍스트를 출력하는 기본 코드 예제입니다:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 음성 파일 열기\n",
    "audio_file = open('data/speech.mp3', 'rb')\n",
    "# 2. whisper API 음성 필사 요청\n",
    "response = client.audio.transcriptions.create(\n",
    "    file = audio_file,\n",
    "    model = \"whisper-1\",\n",
    "    response_format=\"text\"\n",
    ")\n",
    "# 3. 필사된 text 출력\n",
    "print(response)\n",
    "audio_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a383c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Whisper is an advanced speech recognition model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/speech.mp3', 'rb') as audio_file:\n",
    "    response = client.audio.transcriptions.create(\n",
    "        file = audio_file,\n",
    "        model = \"whisper-1\",\n",
    "        response_format=\"text\"\n",
    "    )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c425618",
   "metadata": {},
   "source": [
    "위 코드가 실행되면, speech.mp3 파일의 음성이 텍스트로 변환되어 response에 담기고, 그 내용을 출력합니다. 예를 들어 입력 음성이 영어로 \"OpenAI Whisper is an advanced speech recognition model.\" 라는 문장을 담고 있었다면, 출력은 다음과 같은 텍스트가 될 것입니다:\n",
    "```\n",
    "OpenAI Whisper is an advanced speech recognition model.\n",
    "```\n",
    "이제 Whisper API의 기본 동작을 확인했으므로, 추가적인 기능들과 상황별 활용법을 알아보겠습니다.\n",
    "\n",
    "## 3. 다양한 언어의 음성 인식 및 언어감지\n",
    "\n",
    "Whisper 모델은 다국어 음성 인식을 지원합니다. 90개가 넘는 언어로 훈련되어 다양한 언어의 음성을 텍스트로 변환할 수 있습니다. 기본적으로 API에 언어를 명시하지 않으면 Whisper가 음성의 언어를 자동으로 감지하여 해당 언어로 텍스트를 필사합니다. 예를 들어, 한국어 음성을 입력하면 자동으로 한국어로 텍스트를 출력합니다. \n",
    "\n",
    "언어 감지 및 지정 (language 파라미터):\n",
    "\n",
    "자동 감지가 대부분 잘 동작하지만, 경우에 따라 잘못된 언어로 인식될 가능성도 있습니다. Whisper API는 이런 경우를 대비해 language 파라미터로 입력 음성의 언어를 직접 지정하는 기능을 제공합니다. language 파라미터에 ISO 언어 코드를 설정하면 모델이 해당 언어로 인식하도록 강제할 수 있습니다.\n",
    "- 예를 들어, 한국어 음성 파일을 확실히 한국어로 인식시키고 싶다면 language=\"ko\"로 지정할 수 있습니다.\n",
    "- 영어라면 language=\"en\", 스페인어라면 language=\"es\" 등으로 언어 코드를 설정합니다.\n",
    "\n",
    "다음은 스페인어 음성 파일을 Whisper로 변환하는 코드 예시입니다. Whisper가 자동으로 언어를 감지하게 할 수도 있지만, 예시에서는 language=\"es\"를 명시적으로 지정해보겠습니다:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c4da1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Qué crees que es la inteligencia artificial?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스페인어 필사\n",
    "with open('data/spanish_audio.mp3', \"rb\") as audio_file:\n",
    "    response = client.audio.transcriptions.create(\n",
    "        file=audio_file,\n",
    "        model = 'whisper-1',\n",
    "        response_format=\"text\",\n",
    "        language=\"es\" # 음성 언어를 지정\n",
    "    )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3eb25ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you think artificial intelligence is?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 음성을 번역(영어번역만 가능)\n",
    "with open('data/spanish_audio.mp3', \"rb\") as audio_file:\n",
    "    response = client.audio.translations.create(\n",
    "        file=audio_file,\n",
    "        model = 'whisper-1',\n",
    "        response_format=\"text\"\n",
    "    )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ccb888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성을 한국어로 번역(2단계) : 음성->스페인어필사->한국어\n",
    "with open('data/spanish_audio.mp3', 'rb') as audio_file:\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        file  = audio_file,\n",
    "        model = 'whisper-1',\n",
    "        response_format='text',\n",
    "        language='es'\n",
    "    )\n",
    "# 스페인어를 한국어번역\n",
    "response = client.chat.completions.create(\n",
    "    model = 'gpt-4.1-nano',\n",
    "    messages= [{\n",
    "        'role':'user', \n",
    "        'content' : f'다음 스페인어를 한국어로 번역해 주세요 : {transcription}'\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c22d40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인공지능이 무엇이라고 생각하세요?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c6810be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 저는 OpenAPI로 생성된 한국어 음성입니다. 한국어로 자동인식되어 읽고 있습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 한국어 필사\n",
    "with open('data/output_ko_onyx.mp3', 'rb') as audio_file:\n",
    "    transcription_ko = client.audio.transcriptions.create(\n",
    "        model = 'whisper-1',\n",
    "        file  = audio_file,\n",
    "        response_format='text',\n",
    "        language='ko'\n",
    "    )\n",
    "print(transcription_ko)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b120ddc2",
   "metadata": {},
   "source": [
    "만약 spanish_audio.mp3에 스페인어로 \"¿Qué crees que es la inteligencia artificial?\" 라는 문장이 담겨 있었다면, 위 코드의 출력은 다음과 같이 스페인어 텍스트가 될 것입니다:\n",
    "```\n",
    "¿Qué crees que es la inteligencia artificial?\n",
    "```\n",
    "\n",
    "Whisper에 언어를 지정함으로써 모델이 해당 언어로 정확히 인식하도록 도울 수 있습니다. 대부분의 경우 언어를 지정하지 않아도 잘 동작하지만, 언어 감지 결과를 신뢰하기 어렵거나 특정 언어로 출력되길 원한다면 language 파라미터를 활용하세요.\n",
    "\n",
    "## 4. 고급 기능 1: 타임스탬프 포함 자막 및 세부 출력\n",
    "\n",
    "기본적인 Whisper API 응답은 전체 음성을 하나의 텍스트로 반환하지만, 경우에 따라 문장별로 시간 구간을 표시하거나 단어별 타임스탬프가 필요한 경우도 있습니다 (예: 동영상 자막 생성, 정교한 음성 분석 등). Whisper API는 이러한 요구에 맞게 여러 가지 출력 형식을 제공합니다.\n",
    "\n",
    "### 다양한 출력 형식 (response_format 옵션)\n",
    "response_format 파라미터를 조정하여 Whisper의 출력 형식을 바꿀 수 있습니다:\n",
    "\n",
    "- \"json\" (기본값): { \"text\": \"...\" } 형태의 JSON 응답. 추가 메타정보 없이 최종 텍스트만 포함합니다.\n",
    "- \"text\": 순수 텍스트로 변환 결과만 반환. (이미 앞서 사용)\n",
    "- \"srt\": SubRip 자막 형식으로 반환. 자막처럼 각 문장에 시간 코드가 포함됩니다.\n",
    "- \"vtt\": WebVTT 자막 형식으로 반환. (srt와 유사한 자막 표준 형식)\n",
    "- \"verbose_json\": 인식 결과에 대한 상세 정보를 담은 JSON으로 반환. 인식된 문장별 세그먼트와 타임스탬프, 확률 등 메타데이터가 모두 포함됩니다.\n",
    "\n",
    "자막 형식 출력: 예를 들어 음성 파일을 response_format=\"srt\"으로 요청하면, Whisper는 출력 텍스트를 일정 길이로 나누어 각 부분에 시작-끝 시간이 포함된 자막 형식으로 돌려줍니다. \n",
    "\n",
    "다음 코드를 통해 이전과 동일한 speech.mp3에 대해 SRT 형식 출력을 받아보겠습니다:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "970b61bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:00,000 --> 00:00:04,000\n",
      "OpenAI Whisper is an advanced speech recognition model.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/speech.mp3', 'rb') as audio_file:\n",
    "    response = client.audio.transcriptions.create(\n",
    "        file = audio_file,\n",
    "        model = \"whisper-1\",\n",
    "        response_format=\"srt\"\n",
    "    )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa906033",
   "metadata": {},
   "source": [
    "응답은 문자열 형태로 SRT 콘텐츠가 반환됩니다.\n",
    "\n",
    "위 예시처럼, SRT 포맷에서는 각 블록마다 순번, 시작/끝 시간, 그리고 해당 구간의 대사가 나타납니다. Whisper API가 자동으로 음성을 몇 초 단위로 구분해 자막으로 만들어준 것을 확인할 수 있습니다.\n",
    "\n",
    "세부 JSON 출력: 더 구조적인 정보가 필요하다면 response_format=\"verbose_json\"을 사용할 수 있습니다. 이 모드에서는 음성 전체를 몇 개의 **세그먼트(segment)**로 나누고, 각 세그먼트에 대한 텍스트, 시작/종료 시간 정보(timestamp), 신뢰도 등의 정보를 JSON 형식으로 제공합니다. 이를 활용하면 필요에 따라 세그먼트를 직접 처리하거나 자막을 커스터마이징할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e70bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TranscriptionVerbose(duration=8.970000267028809, language='korean', text='안녕하세요. 저는 OpenAPI로 생성된 한국어 음성입니다. 한국어로 자동인식되어 읽고 있습니다.', segments=[TranscriptionSegment(id=0, avg_logprob=-0.3119690418243408, compression_ratio=1.0598291158676147, end=5.880000114440918, no_speech_prob=0.016902640461921692, seek=0, start=0.0, temperature=0.0, text=' 안녕하세요. 저는 OpenAPI로 생성된 한국어 음성입니다.', tokens=[50364, 19289, 13, 10551, 7238, 4715, 40, 12888, 6439, 8631, 27930, 21045, 3103, 15179, 8631, 7416, 13, 50658]), TranscriptionSegment(id=1, avg_logprob=-0.3119690418243408, compression_ratio=1.0598291158676147, end=8.880000114440918, no_speech_prob=0.016902640461921692, seek=0, start=5.880000114440918, temperature=0.0, text=' 한국어로 자동인식되어 읽고 있습니다.', tokens=[50658, 21045, 6540, 1955, 15905, 8309, 4215, 10436, 22826, 3103, 43302, 1313, 10552, 13, 50808])], usage=Usage(seconds=9.0, type='duration'), words=None, task='transcribe')\n"
     ]
    }
   ],
   "source": [
    "with open('data/output_ko_onyx.mp3', 'rb') as audio_file:\n",
    "    transcription_ko = client.audio.transcriptions.create(\n",
    "        model = 'whisper-1',\n",
    "        file  = audio_file,\n",
    "        response_format='verbose_json',\n",
    "        language='ko'\n",
    "    )\n",
    "print(transcription_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21832d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 구간 : 0.0 => 5.880000114440918\n",
      "text :  안녕하세요. 저는 OpenAPI로 생성된 한국어 음성입니다.\n"
     ]
    }
   ],
   "source": [
    "seg = transcription_ko.segments[0]\n",
    "print(\"text 구간 : {} => {}\".format(seg.start, seg.end))\n",
    "print(\"text :\", seg.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f29b5",
   "metadata": {},
   "source": [
    "위 코드에서 transcription_json은 segments에 여러 세그먼트가 리스트로 포함됩니다. 각 세그먼트에는 \"start\", \"end\" (초 단위 시간), \"text\" 등이 담겨 있으며, 이 외에도 \"confidence\"(평균 예측 확률)나 \"language\"(감지된 언어) 등의 부가 정보가 있을 수 있습니다.\n",
    "\n",
    "## 5. 고급 기능 2: 실시간 음성 변환 (마이크 입력 처리)\n",
    "\n",
    "지금까지는 파일에 저장된 음성을 변환하는 방법을 다루었습니다. 이번에는 실시간 마이크 입력을 받아서 Whisper API로 전송하는 방법을 살펴보겠습니다. 실시간 처리란 말 그대로 사용자가 말하는 동시에(text) 바로바로 텍스트로 변환하는 것을 의미합니다. Whisper API는 스트리밍 엔드포인트를 제공하지는 않으나, 짧은 구간으로 녹음하여 연속으로 API에 보내는 방식으로 유사 실시간 처리가 가능합니다.\n",
    ">주의: 네트워크 API를 이용하는 이상 완벽한 실시간(동시에 변환) 처리에는 한계가 있습니다. 음성을 일정 간격으로 끊어서 전송하면 약간의 지연은 발생하지만, 빠른 응답 속도를 감안하면 거의 실시간에 가까운 자막을 구현할 수 있습니다. 진정한 실시간 처리가 필요하다면 오픈소스 Whisper 모델을 로컬에서 사용하거나 스트리밍 기능을 지원하는 서비스를 검토해야 합니다.\n",
    "\n",
    "### 마이크 입력 녹음하여 전송하기\n",
    "Python에서는 sounddevice나 pyaudio 등의 라이브러리를 사용하여 마이크로부터 오디오 데이터를 받을 수 있습니다. 여기서는 sounddevice를 활용한 간단한 예제를 소개합니다:\n",
    "\n",
    "1. 짧은 음성 녹음: sounddevice.rec 함수를 사용하면 지정한 초 만큼 마이크로부터 오디오를 녹음할 수 있습니다. 아래 예시는 5초간 모노 음성을 16kHz로 녹음합니다.\n",
    "2. 오디오 데이터를 파일로 저장: Whisper API에 전송하려면 오디오 데이터를 파일 형태 (또는 파일 객체)로 제공해야 합니다. scipy.io.wavfile.write 함수를 이용해 녹음한 데이터를 WAV 파일로 저장합니다.\n",
    "3. 저장한 파일 전송: 앞서와 동일하게 client.audio.transcriptions.create를 사용하여 Whisper API로 WAV 파일을 보내면 됩니다.\n",
    "\n",
    "코드 예시는 다음과 같습니다 (5초간 녹음 후 즉시 전송):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "fs = 16000 # 녹음될 음성 데이터 샘플레이트 16kHz\n",
    "seconds = 5\n",
    "print('지금부터 5초간 녹음됩니다')\n",
    "recording = sd.rec(int(seconds*fs), samplerate=fs, channels=1, dtype='int16')\n",
    "sd.wait() # 녹음이 끝날 때까지 코드 실행을 멈춤\n",
    "file_name = 'data/ch6_live_input.wav'\n",
    "write(file_name, recording)\n",
    "print('녹음된 음성이 저장되었습니다. 음성파일 크기 :', len(recording))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bedd25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9be27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb125a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427111f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d168ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265db50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd267f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905ec7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm(ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
